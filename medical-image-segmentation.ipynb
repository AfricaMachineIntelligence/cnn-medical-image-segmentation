{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import glob\n",
    "import os.path \n",
    "import cv2\n",
    "import numpy as np \n",
    "import collections\n",
    "import matplotlib\n",
    "import scipy.spatial.distance\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt \n",
    "import matplotlib.animation as animation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# how to load tif images\n",
    "IMAGE_DIR        = 'train_subset'\n",
    "MSEC_PER_FRAME   = 200  \n",
    "MSEC_REPEAT_DELAY= 2000\n",
    "ADD_MASK_OUTLINE = True\n",
    "TILE_MIN_SIDE    = 50     # pixels; see tile_features()\n",
    "SHOW_GIF         = False  # matplotlib popup of animation \n",
    "\n",
    "\n",
    "def get_image(f):\n",
    "    # Read image file \n",
    "    img = cv2.imread(f)\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    # print 'Read:', f\n",
    "    return img\n",
    "\n",
    "\n",
    "def grays_to_RGB(img):\n",
    "    # Convert a 1-channel grayscale image into 3 channel RGB image\n",
    "    return np.dstack((img, img, img))\n",
    "\n",
    "\n",
    "def image_plus_mask(img, mask):\n",
    "    # Returns a copy of the grayscale image, converted to RGB, \n",
    "    # and with the edges of the mask added in red\n",
    "    img_color = grays_to_RGB(img)\n",
    "    mask_edges = cv2.Canny(mask, 100, 200) > 0  \n",
    "    img_color[mask_edges, 0] = 255  # chan 0 = bright red\n",
    "    img_color[mask_edges, 1] = 0\n",
    "    img_color[mask_edges, 2] = 0\n",
    "    return img_color\n",
    "\n",
    "\n",
    "def to_mask_path(f_image):\n",
    "    # Convert an image file path into a corresponding mask file path \n",
    "    dirname, basename = os.path.split(f_image)\n",
    "    maskname = basename.replace(\".tif\", \"_mask.tif\")\n",
    "    return os.path.join(dirname, maskname)\n",
    "\n",
    "\n",
    "def add_masks(images):\n",
    "    # Return copies of the group of images with mask outlines added\n",
    "    # Images are stored as dict[filepath], output is also dict[filepath]\n",
    "    images_plus_masks = {} \n",
    "    for f_image in images:\n",
    "        img  = images[f_image]\n",
    "        mask = cv2.imread(to_mask_path(f_image))\n",
    "        images_plus_masks[f_image] = image_plus_mask(img, mask)\n",
    "    return images_plus_masks\n",
    "\n",
    "\n",
    "def get_patient_images(patient):\n",
    "    # Return a dict of patient images, i.e. dict[filepath]\n",
    "    f_path = IMAGE_DIR + '%i_*.tif' % patient \n",
    "    f_ultrasounds = [f for f in glob.glob(f_path) if 'mask' not in f]\n",
    "    images = {f:get_image(f) for f in f_ultrasounds}\n",
    "    return images\n",
    "\n",
    "\n",
    "def image_features(img):\n",
    "    return tile_features(img)   # a tile is just an image...\n",
    "\n",
    "\n",
    "def tile_features(tile, tile_min_side = TILE_MIN_SIDE):\n",
    "    # Recursively split a tile (image) into quadrants, down to a minimum \n",
    "    # tile size, then return flat array of the mean brightness in those tiles.\n",
    "    tile_x, tile_y = tile.shape\n",
    "    mid_x = tile_x / 2\n",
    "    mid_y = tile_y / 2\n",
    "    if (mid_x < tile_min_side) or (mid_y < tile_min_side):\n",
    "        return np.array([tile.mean()]) # hit minimum tile size\n",
    "    else:\n",
    "        tiles = [ tile[:mid_x, :mid_y ], tile[mid_x:, :mid_y ], \n",
    "                  tile[:mid_x , mid_y:], tile[mid_x:,  mid_y:] ] \n",
    "        features = [tile_features(t) for t in tiles]\n",
    "        return np.array(features).flatten()\n",
    "\n",
    "\n",
    "def feature_dist(feats_0, feats_1):\n",
    "    # Definition of the distance metric between image features\n",
    "    return scipy.spatial.distance.euclidean(feats_0, feats_1)\n",
    "\n",
    "\n",
    "def feature_dists(features):\n",
    "    # Calculate the distance between all pairs of images (using their features)\n",
    "    dists = collections.defaultdict(dict)\n",
    "    f_img_features = features.keys()\n",
    "    for f_img0, f_img1 in itertools.permutations(f_img_features, 2):\n",
    "        dists[f_img0][f_img1] = feature_dist(features[f_img0], features[f_img1])\n",
    "    return dists\n",
    "\n",
    "\n",
    "def image_seq_start(dists, f_start):\n",
    "\n",
    "    # Given a starting image (i.e. named f_start), greedily pick a sequence \n",
    "    # of nearest-neighbor images until there are no more unpicked images. \n",
    "\n",
    "    f_picked = [f_start]\n",
    "    f_unpicked = set(dists.keys()) - set([f_start])\n",
    "    f_current = f_start\n",
    "    dist_tot = 0\n",
    "\n",
    "    while f_unpicked:\n",
    "\n",
    "        # Collect the distances from the current image to the \n",
    "        # remaining unpicked images, then pick the nearest one \n",
    "        candidates = [(dists[f_current][f_next], f_next) for f_next in f_unpicked]\n",
    "        dist_nearest, f_nearest = list(sorted(candidates))[0]\n",
    "\n",
    "        # Update the image accounting & make the nearest image the current image \n",
    "        f_unpicked.remove(f_nearest)\n",
    "        f_picked.append(f_nearest)\n",
    "        dist_tot += dist_nearest\n",
    "        f_current = f_nearest \n",
    "\n",
    "    return (dist_tot, f_picked)\n",
    "\n",
    "\n",
    "def image_sequence(dists):\n",
    "\n",
    "    # Return a sequence of images that minimizes the sum of \n",
    "    # inter-image distances. This function relies on image_seq_start(), \n",
    "    # which requires an arbitray starting image. \n",
    "    # In order to find an even lower-cost sequence, this function\n",
    "    # tries all possible staring images and returns the best result.\n",
    "\n",
    "    f_starts = dists.keys()\n",
    "    seqs = [image_seq_start(dists, f_start) for f_start in f_starts]\n",
    "    dist_best, seq_best = list(sorted(seqs))[0]\n",
    "    return seq_best\n",
    "\n",
    "\n",
    "def grayscale_to_RGB(img):\n",
    "    return np.asarray(np.dstack((img, img, img)), dtype=np.uint8)\n",
    "\n",
    "\n",
    "def build_gif(imgs, fname, show_gif=True, save_gif=True, title=''):\n",
    "    # Create an animated GIF file from a sequence of images\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111)\n",
    "    ax.set_axis_off()\n",
    "    fig.subplots_adjust(left=0, bottom=0, right=1, top=1, \n",
    "                        wspace=None, hspace=None)  # removes white border\n",
    "    #imgs = [(ax.imshow(img), ax.set_title(title)) for img in imgs] \n",
    "    imgs = [ (ax.imshow(img), \n",
    "              ax.set_title(title), \n",
    "              ax.annotate(n_img,(5,5))) for n_img, img in enumerate(imgs) ] \n",
    "\n",
    "    img_anim = animation.ArtistAnimation(fig, imgs, interval=MSEC_PER_FRAME, \n",
    "                                repeat_delay=MSEC_REPEAT_DELAY, blit=False)\n",
    "    if save_gif:\n",
    "        print('Writing:', fname)\n",
    "        img_anim.save(fname, writer='imagemagick')\n",
    "    if show_gif:\n",
    "        plt.show()\n",
    "    plt.clf() # clearing the figure when done prevents a memory leak \n",
    "\n",
    "\n",
    "def write_gif(f_seq, images, fname):\n",
    "    imgs = [images[f] for f in f_seq] # get images indexed by their filenames\n",
    "    build_gif(imgs, fname, show_gif=SHOW_GIF)\n",
    "\n",
    "\n",
    "def write_patient_video(patient):\n",
    "    # Given a patient number, create an animaged GIF of their ultrasounds\n",
    "    # including an outline of any mask created that identifies nerve tissue.\n",
    "    images       = get_patient_images(patient=patient)\n",
    "    images_masks = add_masks(images)\n",
    "    features     = { f : image_features(images[f]) for f in images }\n",
    "    dists        = feature_dists(features)\n",
    "    f_seq        = image_sequence(dists)\n",
    "    write_gif(f_seq, images_masks, 'patient-%02i.gif' % patient)\n",
    "\n",
    "\n",
    "def main():\n",
    "\n",
    "    # Animations for patients 32 and 41 are particularly good examples. \n",
    "    write_patient_video(patient=41)\n",
    "    write_patient_video(patient=32)\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# convert image to image_tensor in tf\n",
    "\n",
    "# downsample and upsample\n",
    "\n",
    "# resizing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN model architecture\n",
    "\n",
    "Xception\n",
    "\n",
    "VGG16\n",
    "\n",
    "VGG19\n",
    "\n",
    "ResNet50\n",
    "\n",
    "InceptionV3\n",
    "\n",
    "InceptionResNetV2\n",
    "\n",
    "MobileNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# grayscale\n",
    "\n",
    "# segmentation, binary classification, pixel-wise classification\n",
    "\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "slim = tf.contrib.slim\n",
    "\n",
    "\n",
    "def vgg_arg_scope(weight_decay=0.0005):\n",
    "  \"\"\"Defines the VGG arg scope.\n",
    "\n",
    "  Args:\n",
    "    weight_decay: The l2 regularization coefficient.\n",
    "\n",
    "  Returns:\n",
    "    An arg_scope.\n",
    "  \"\"\"\n",
    "  with slim.arg_scope([slim.conv2d, slim.fully_connected],\n",
    "                      activation_fn=tf.nn.relu,\n",
    "                      weights_regularizer=slim.l2_regularizer(weight_decay),\n",
    "                      biases_initializer=tf.zeros_initializer()):\n",
    "    with slim.arg_scope([slim.conv2d], padding='SAME') as arg_sc:\n",
    "      return arg_sc\n",
    "\n",
    "\n",
    "def mis_16(inputs,\n",
    "           num_classes=2,\n",
    "           is_training=True,\n",
    "           dropout_keep_prob=0.5,\n",
    "           spatial_squeeze=True,\n",
    "           scope='mis_16',\n",
    "           fc_conv_padding='SAME'):\n",
    "  \"\"\"\n",
    "  Note: All the fully_connected layers have been transformed to conv2d layers.\n",
    "        To use in classification mode, resize input to 224x224.\n",
    "\n",
    "  Args:\n",
    "    inputs: a tensor of size [batch_size, height, width, channels].\n",
    "    num_classes: number of predicted classes.\n",
    "    is_training: whether or not the model is being trained.\n",
    "    dropout_keep_prob: the probability that activations are kept in the dropout\n",
    "      layers during training.\n",
    "    spatial_squeeze: whether or not should squeeze the spatial dimensions of the\n",
    "      outputs. Useful to remove unnecessary dimensions for classification.\n",
    "    scope: Optional scope for the variables.\n",
    "    fc_conv_padding: the type of padding to use for the fully connected layer\n",
    "      that is implemented as a convolutional layer. Use 'SAME' padding if you\n",
    "      are applying the network in a fully convolutional manner and want to\n",
    "      get a prediction map downsampled by a factor of 32 as an output.\n",
    "      Otherwise, the output prediction map will be (input / 32) - 6 in case of\n",
    "      'VALID' padding.\n",
    "\n",
    "  Returns:\n",
    "    the last op containing the log predictions and end_points dict.\n",
    "  \"\"\"\n",
    "\n",
    "  with tf.variable_scope(scope, 'mis_16', [inputs]) as sc:\n",
    "    end_points_collection = sc.name + '_end_points'\n",
    "    # Collect outputs for conv2d, fully_connected and max_pool2d.\n",
    "    with slim.arg_scope([slim.conv2d, slim.fully_connected, slim.max_pool2d],\n",
    "                        outputs_collections=end_points_collection):\n",
    "      net = slim.repeat(inputs, 2, slim.conv2d, 64, [3, 3], scope='conv1')\n",
    "      net = slim.max_pool2d(net, [2, 2], scope='pool1')\n",
    "      net = slim.repeat(net, 2, slim.conv2d, 128, [3, 3], scope='conv2')\n",
    "      net = slim.max_pool2d(net, [2, 2], scope='pool2')\n",
    "      net = slim.repeat(net, 3, slim.conv2d, 256, [3, 3], scope='conv3')\n",
    "      net = slim.max_pool2d(net, [2, 2], scope='pool3')\n",
    "      net = slim.repeat(net, 3, slim.conv2d, 512, [3, 3], scope='conv4')\n",
    "      net = slim.max_pool2d(net, [2, 2], scope='pool4')\n",
    "      net = slim.repeat(net, 3, slim.conv2d, 512, [3, 3], scope='conv5')\n",
    "      net = slim.max_pool2d(net, [2, 2], scope='pool5')\n",
    "      # Use conv2d instead of fully_connected layers.\n",
    "      net = slim.conv2d(net, 4096, [7, 7], padding=fc_conv_padding, scope='fc6')\n",
    "      net = slim.dropout(net, dropout_keep_prob, is_training=is_training,\n",
    "                         scope='dropout6')\n",
    "      net = slim.conv2d(net, 4096, [1, 1], scope='fc7')\n",
    "      net = slim.dropout(net, dropout_keep_prob, is_training=is_training,\n",
    "                         scope='dropout7')\n",
    "      net = slim.conv2d(net, num_classes, [1, 1],\n",
    "                        activation_fn=None,\n",
    "                        normalizer_fn=None,\n",
    "                        scope='fc8')\n",
    "      # Convert end_points_collection into a end_point dict.\n",
    "      end_points = slim.utils.convert_collection_to_dict(end_points_collection)\n",
    "      if spatial_squeeze:\n",
    "        net = tf.squeeze(net, [1, 2], name='fc8/squeezed')\n",
    "        end_points[sc.name + '/fc8'] = net\n",
    "      return net, end_points\n",
    "vgg_16.default_image_size = 224\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Masked image augmentation demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(70, 116, 3)\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n",
    "\n",
    "datagen = ImageDataGenerator(\n",
    "        rotation_range=180,\n",
    "        width_shift_range=0,\n",
    "        height_shift_range=0,\n",
    "        shear_range=0,\n",
    "        zoom_range=0,\n",
    "        horizontal_flip=True,\n",
    "        fill_mode='nearest')\n",
    "\n",
    "img = load_img('data_simple_cnn/train/mask/1_mask.jpg')  # this is a PIL image\n",
    "x = img_to_array(img)  # this is a Numpy array with shape (70, 116, 3)\n",
    "\n",
    "print(x.shape)\n",
    "\n",
    "x = x.reshape((1,) + x.shape)  # this is a Numpy array with shape (1, 3, 150, 150)\n",
    "\n",
    "# the .flow() command below generates batches of randomly transformed images\n",
    "# and saves the results to the `preview/` directory\n",
    "i = 0\n",
    "for batch in datagen.flow(x, batch_size=1,\n",
    "                          save_to_dir='aug_preview', save_prefix='mask', save_format='jpeg'):\n",
    "    i += 1\n",
    "    if i > 20:\n",
    "        break  # otherwise the generator would loop indefinitely"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Simple CNN model architecture for classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.layers import Activation, Dropout, Flatten, Dense\n",
    "\n",
    "simple_cnn = Sequential()\n",
    "simple_cnn.add(Conv2D(16, (3, 3), input_shape=(70, 116, 1), padding='same', activation='relu', name='conv1_1'))\n",
    "simple_cnn.add(Conv2D(16, (3, 3), input_shape=(70, 116, 1), padding='same', activation='relu', name='conv1_2'))\n",
    "simple_cnn.add(MaxPooling2D(pool_size=(2, 2), name='pool1'))\n",
    "\n",
    "simple_cnn.add(Conv2D(32, (3, 3), padding='same', activation='relu', name='conv2_1'))\n",
    "simple_cnn.add(Conv2D(32, (3, 3), padding='same', activation='relu', name='conv2_2'))\n",
    "simple_cnn.add(MaxPooling2D(pool_size=(5, 2), name='pool2'))\n",
    "\n",
    "simple_cnn.add(Flatten(name='flatten'))\n",
    "simple_cnn.add(Dense(32, activation='relu', name='fc1'))\n",
    "simple_cnn.add(Dropout(0.5))\n",
    "simple_cnn.add(Dense(1, activation='sigmoid', name='prediction'))\n",
    "\n",
    "simple_cnn.compile(loss='binary_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1_1 (Conv2D)             (None, 70, 116, 16)       160       \n",
      "_________________________________________________________________\n",
      "conv1_2 (Conv2D)             (None, 70, 116, 16)       2320      \n",
      "_________________________________________________________________\n",
      "pool1 (MaxPooling2D)         (None, 35, 58, 16)        0         \n",
      "_________________________________________________________________\n",
      "conv2_1 (Conv2D)             (None, 35, 58, 32)        4640      \n",
      "_________________________________________________________________\n",
      "conv2_2 (Conv2D)             (None, 35, 58, 32)        9248      \n",
      "_________________________________________________________________\n",
      "pool2 (MaxPooling2D)         (None, 7, 29, 32)         0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 6496)              0         \n",
      "_________________________________________________________________\n",
      "fc1 (Dense)                  (None, 32)                207904    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "prediction (Dense)           (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 224,305.0\n",
      "Trainable params: 224,305.0\n",
      "Non-trainable params: 0.0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "simple_cnn.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Read small image files for classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 14400 images belonging to 2 classes.\n",
      "Found 3570 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "train_datagen = ImageDataGenerator(\n",
    "        rotation_range=180,\n",
    "        horizontal_flip=True,\n",
    "        fill_mode='nearest')\n",
    "\n",
    "test_datagen = ImageDataGenerator(rotation_range=180, horizontal_flip=True)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "        'data_simple_cnn/train',  \n",
    "        target_size=(70, 116), \n",
    "        batch_size=batch_size,\n",
    "        class_mode='binary',\n",
    "        color_mode='grayscale') \n",
    "\n",
    "validation_generator = test_datagen.flow_from_directory(\n",
    "        'data_simple_cnn/validation',\n",
    "        target_size=(70, 116),\n",
    "        batch_size=batch_size,\n",
    "        class_mode='binary',\n",
    "        color_mode='grayscale')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Classification training for small images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "7/7 [==============================] - 6s - loss: 0.5567 - acc: 0.9554 - val_loss: 0.2491 - val_acc: 0.9844\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 2/10\n",
      "7/7 [==============================] - 5s - loss: 0.3559 - acc: 0.9777 - val_loss: 0.2491 - val_acc: 0.9844\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 3/10\n",
      "7/7 [==============================] - 5s - loss: 0.5722 - acc: 0.9643 - val_loss: 0.1245 - val_acc: 0.9922\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 4/10\n",
      "7/7 [==============================] - 5s - loss: 0.6405 - acc: 0.9598 - val_loss: 0.1245 - val_acc: 0.9922\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 5/10\n",
      "7/7 [==============================] - 5s - loss: 0.2847 - acc: 0.9821 - val_loss: 1.1921e-07 - val_acc: 1.0000\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 6/10\n",
      "7/7 [==============================] - 5s - loss: 0.2847 - acc: 0.9821 - val_loss: 0.1245 - val_acc: 0.9922\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 7/10\n",
      "7/7 [==============================] - 5s - loss: 0.6431 - acc: 0.9598 - val_loss: 0.4982 - val_acc: 0.9688\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 8/10\n",
      "7/7 [==============================] - 5s - loss: 0.4270 - acc: 0.9732 - val_loss: 0.1245 - val_acc: 0.9922\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 9/10\n",
      "7/7 [==============================] - 5s - loss: 0.6405 - acc: 0.9598 - val_loss: 0.4982 - val_acc: 0.9688\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 10/10\n",
      "7/7 [==============================] - 5s - loss: 0.3559 - acc: 0.9777 - val_loss: 0.1245 - val_acc: 0.9922\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n"
     ]
    }
   ],
   "source": [
    "simple_cnn.fit_generator(\n",
    "        train_generator,\n",
    "        steps_per_epoch=224 // batch_size,\n",
    "        epochs=10,\n",
    "        validation_data=validation_generator,\n",
    "        validation_steps=112/ batch_size)\n",
    "simple_cnn.save_weights('simple_cnn.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image segmention loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import keras.backend as K\n",
    "\n",
    "def binary_crossentropy_with_logits(ground_truth, predictions):\n",
    "    return K.mean(K.binary_crossentropy(ground_truth,\n",
    "                                        predictions,\n",
    "                                        from_logits=True),\n",
    "                  axis=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### FCN model architecture for small image segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Conv2D, MaxPooling2D, Conv2DTranspose, Add, Dropout\n",
    "\n",
    "'''\n",
    "simple_cnn = Sequential()\n",
    "simple_cnn.add(Conv2D(32, (3, 3), input_shape=(70, 116, 1), padding='same', activation='relu', name='conv1'))\n",
    "simple_cnn.add(MaxPooling2D(pool_size=(2, 2), name='pool1'))\n",
    "\n",
    "simple_cnn.add(Conv2D(64, (3, 3), padding='same', activation='relu', name='conv2'))\n",
    "simple_cnn.add(MaxPooling2D(pool_size=(5, 2), name='pool2'))\n",
    "\n",
    "simple_cnn.add(Flatten(name='flatten'))\n",
    "simple_cnn.add(Dense(64, activation='relu', name='fc1'))\n",
    "simple_cnn.add(Dropout(0.5))\n",
    "simple_cnn.add(Dense(1, activation='sigmoid', name='prediction'))\n",
    "'''\n",
    "def fcn_cnn():\n",
    "    fc_cnn = Sequential()\n",
    "    fc_cnn.add(Conv2D(16, (3, 3), input_shape=(70, 116, 1), padding='same', activation='relu', name='conv1_1'))\n",
    "    fc_cnn.add(Conv2D(16, (3, 3), input_shape=(70, 116, 1), padding='same', activation='relu', name='conv1_2'))\n",
    "    fc_cnn.add(MaxPooling2D(pool_size=(2, 2), name='pool1'))\n",
    "\n",
    "    fc_cnn.add(Conv2D(32, (3, 3), padding='same', activation='relu', name='conv2_1'))\n",
    "    fc_cnn.add(Conv2D(32, (3, 3), padding='same', activation='relu', name='conv2_2'))\n",
    "    fc_cnn.add(MaxPooling2D(pool_size=(5, 2), name='pool2'))\n",
    "\n",
    "    # continue to use convoluational layers instead of fully connected layers\n",
    "    fc_cnn.add(Conv2D(128, (7, 29), padding='same', activation='relu', name='fc3'))\n",
    "    fc_cnn.add(Dropout(0.5))\n",
    "    fc_cnn.add(Conv2D(128, (1, 1), padding='same', activation='relu', name='fc4'))\n",
    "    fc_cnn.add(Dropout(0.5))\n",
    "\n",
    "    # remove original final sigmoid classifier\n",
    "    fc_cnn.add(Conv2D(2, (1, 1), padding='same', name='logit_fc4')) # [7, 29, 2]\n",
    "    # deconv logit_fc4 by [5, 2] to [35, 58, 2]\n",
    "    fc_cnn.add(Conv2DTranspose(2, kernel_size=(2*5-5%2, 2*2-2%2), strides=(5, 2), padding='same', name='deconv_logit_fc4'))\n",
    "    # conv logit from pool1 to [35, 58, 2]\n",
    "\n",
    "    # add deconv_logits_fc4 and logit_pool1\n",
    "    logit_pool1 = Conv2D(2, (1, 1), padding='same', name='logit_pool1')(fc_cnn.layers[2].output)\n",
    "    logit_pool1_deconv_logit_fc4 = Add()([logit_pool1, fc_cnn.layers[-1].output])\n",
    "    # deconv above sum by [2, 2] to [70, 116, 2] --> pixel-wise classification: segmentation logit\n",
    "    final_deconv = Conv2DTranspose(2, kernel_size=(2*2-2%2, 2*2-2%2), strides=(2, 2), \n",
    "                                   padding='same', name='final_deconv')(logit_pool1_deconv_logit_fc4)\n",
    "    \n",
    "    return Model(fc_cnn.input, final_deconv)\n",
    "\n",
    "fcn = fcn_cnn()\n",
    "\n",
    "# need to change loss for image and mask logit cross entropy\n",
    "fcn.compile(loss=binary_crossentropy_with_logits,\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1_1 (Conv2D)             (None, 70, 116, 16)       160       \n",
      "_________________________________________________________________\n",
      "conv1_2 (Conv2D)             (None, 70, 116, 16)       2320      \n",
      "_________________________________________________________________\n",
      "pool1 (MaxPooling2D)         (None, 35, 58, 16)        0         \n",
      "_________________________________________________________________\n",
      "conv2_1 (Conv2D)             (None, 35, 58, 32)        4640      \n",
      "_________________________________________________________________\n",
      "conv2_2 (Conv2D)             (None, 35, 58, 32)        9248      \n",
      "_________________________________________________________________\n",
      "pool2 (MaxPooling2D)         (None, 7, 29, 32)         0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 6496)              0         \n",
      "_________________________________________________________________\n",
      "fc1 (Dense)                  (None, 32)                207904    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "prediction (Dense)           (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 224,305.0\n",
      "Trainable params: 224,305.0\n",
      "Non-trainable params: 0.0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "simple_cnn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "layers = simple_cnn.layers\n",
    "for i in range(len(layers)):\n",
    "    n = len(layers[i].get_weights())\n",
    "    print(str(i) + \"-layer weight len: \", n)\n",
    "    if n == 2:\n",
    "        print(\"  weight matrix size: \", layers[i].get_weights()[0].shape)\n",
    "        print(\"  bias vector size: \", layers[i].get_weights()[1].shape)\n",
    "        print(layers[i].get_weights()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "conv1_1_input (InputLayer)       (None, 70, 116, 1)    0                                            \n",
      "____________________________________________________________________________________________________\n",
      "conv1_1 (Conv2D)                 (None, 70, 116, 16)   160                                          \n",
      "____________________________________________________________________________________________________\n",
      "conv1_2 (Conv2D)                 (None, 70, 116, 16)   2320                                         \n",
      "____________________________________________________________________________________________________\n",
      "pool1 (MaxPooling2D)             (None, 35, 58, 16)    0                                            \n",
      "____________________________________________________________________________________________________\n",
      "conv2_1 (Conv2D)                 (None, 35, 58, 32)    4640                                         \n",
      "____________________________________________________________________________________________________\n",
      "conv2_2 (Conv2D)                 (None, 35, 58, 32)    9248                                         \n",
      "____________________________________________________________________________________________________\n",
      "pool2 (MaxPooling2D)             (None, 7, 29, 32)     0                                            \n",
      "____________________________________________________________________________________________________\n",
      "fc3 (Conv2D)                     (None, 7, 29, 128)    831616                                       \n",
      "____________________________________________________________________________________________________\n",
      "dropout_12 (Dropout)             (None, 7, 29, 128)    0                                            \n",
      "____________________________________________________________________________________________________\n",
      "fc4 (Conv2D)                     (None, 7, 29, 128)    16512                                        \n",
      "____________________________________________________________________________________________________\n",
      "dropout_13 (Dropout)             (None, 7, 29, 128)    0                                            \n",
      "____________________________________________________________________________________________________\n",
      "logit_fc4 (Conv2D)               (None, 7, 29, 2)      258                                          \n",
      "____________________________________________________________________________________________________\n",
      "logit_pool1 (Conv2D)             (None, 35, 58, 2)     34                                           \n",
      "____________________________________________________________________________________________________\n",
      "deconv_logit_fc4 (Conv2DTranspos (None, 35, 58, 2)     146                                          \n",
      "____________________________________________________________________________________________________\n",
      "add_6 (Add)                      (None, 35, 58, 2)     0                                            \n",
      "____________________________________________________________________________________________________\n",
      "final_deconv (Conv2DTranspose)   (None, 70, 116, 2)    66                                           \n",
      "====================================================================================================\n",
      "Total params: 865,000.0\n",
      "Trainable params: 865,000.0\n",
      "Non-trainable params: 0.0\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "fcn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "layers = fcn.layers\n",
    "for i in range(len(layers)):\n",
    "    n = len(layers[i].get_weights())\n",
    "    print(str(i) + \"-layer weight len: \", n)\n",
    "    if n == 2:\n",
    "        print(\"  weight matrix size: \", layers[i].get_weights()[0].shape)\n",
    "        print(\"  bias vector size: \", layers[i].get_weights()[1].shape)\n",
    "        print(layers[i].get_weights()[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize weights in fcn from those in simple_cnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def set_weights(fcn, simple_cnn):\n",
    "    for i in range(5):\n",
    "        fcn.layers[i+1].set_weights(simple_cnn.layers[i].get_weights())\n",
    "        \n",
    "set_weights(fcn, simple_cnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "layers = fcn.layers\n",
    "for i in range(len(layers)):\n",
    "    n = len(layers[i].get_weights())\n",
    "    print(str(i) + \"-layer weight len: \", n)\n",
    "    if n == 2:\n",
    "        print(\"  weight matrix size: \", layers[i].get_weights()[0].shape)\n",
    "        print(\"  bias vector size: \", layers[i].get_weights()[1].shape)\n",
    "        print(layers[i].get_weights()[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read small image and mask files for segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 14400 images belonging to 1 classes.\n",
      "Found 14400 images belonging to 1 classes.\n"
     ]
    }
   ],
   "source": [
    "import skimage.io as io\n",
    "import numpy as np\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "data_gen_args = dict(featurewise_center=True,\n",
    "                     featurewise_std_normalization=True,\n",
    "                     rotation_range=180.,\n",
    "                     horizontal_flip=True,\n",
    "                     fill_mode='nearest')\n",
    "image_datagen = ImageDataGenerator(**data_gen_args)\n",
    "mask_datagen = ImageDataGenerator(**data_gen_args)\n",
    "\n",
    "seed = 1\n",
    "imgs_filename = [\"data_fcn/train/images/images/\"+str(i)+\".jpg\" for i in range(1, 10)]\n",
    "masks_filename = [\"data_fcn/train/masks/masks/\"+str(i)+\"_mask.jpg\" for i in range(1, 10)]\n",
    "sample_imgs = [np.expand_dims(io.imread(img_name), -1) for img_name in imgs_filename]\n",
    "sample_masks = [np.expand_dims(io.imread(mask_name), -1) for mask_name in masks_filename]\n",
    "image_datagen.fit(sample_imgs, augment=True, seed=seed)\n",
    "mask_datagen.fit(sample_masks, augment=True, seed=seed)\n",
    "\n",
    "image_generator = image_datagen.flow_from_directory(\n",
    "    'data_fcn/train/images',\n",
    "    target_size=(70, 116),\n",
    "    batch_size=batch_size,\n",
    "    class_mode=None,\n",
    "    seed=seed,\n",
    "    color_mode='grayscale')\n",
    "\n",
    "mask_generator = mask_datagen.flow_from_directory(\n",
    "    'data_fcn/train/masks',\n",
    "    target_size=(70, 116),\n",
    "    batch_size=batch_size,\n",
    "    class_mode=None,\n",
    "    seed=seed,\n",
    "    color_mode='grayscale')\n",
    "\n",
    "train_generator = zip(image_generator, mask_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_gen_args = dict(featurewise_center=True,\n",
    "                     featurewise_std_normalization=True,\n",
    "                     rotation_range=180.,\n",
    "                     horizontal_flip=True,\n",
    "                     fill_mode='nearest')\n",
    "image_datagen = ImageDataGenerator(**data_gen_args)\n",
    "mask_datagen = ImageDataGenerator(**data_gen_args)\n",
    "\n",
    "seed = 1\n",
    "imgs_filename = [\"data_fcn/validation/images/images/\"+str(i)+\".jpg\" for i in range(1, 10)]\n",
    "masks_filename = [\"data_fcn/validation/masks/masks/\"+str(i)+\"_mask.jpg\" for i in range(1, 10)]\n",
    "sample_imgs = [np.expand_dims(io.imread(img_name), -1) for img_name in imgs_filename]\n",
    "sample_masks = [np.expand_dims(io.imread(mask_name), -1) for mask_name in masks_filename]\n",
    "image_datagen.fit(sample_imgs, augment=True, seed=seed)\n",
    "mask_datagen.fit(sample_masks, augment=True, seed=seed)\n",
    "\n",
    "image_generator = image_datagen.flow_from_directory(\n",
    "    'data_fcn/validation/images',\n",
    "    target_size=(70, 116),\n",
    "    batch_size=batch_size,\n",
    "    class_mode=None,\n",
    "    seed=seed,\n",
    "    color_mode='grayscale')\n",
    "\n",
    "mask_generator = mask_datagen.flow_from_directory(\n",
    "    'data_fcn/validation/masks',\n",
    "    target_size=(70, 116),\n",
    "    batch_size=batch_size,\n",
    "    class_mode=None,\n",
    "    seed=seed,\n",
    "    color_mode='grayscale')\n",
    "\n",
    "train_generator = zip(image_generator, mask_generator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### FCN model training on small images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fcn.fit_generator(\n",
    "    train_generator,\n",
    "    steps_per_epoch=50,\n",
    "    epochs=5,\n",
    "    validation_data=validation_generator,\n",
    "    validation_steps=25)\n",
    "fcn.save_weights('fcn.h5')\n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FCN model prediction on small images demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import skimage.io as io\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "img = io.imread('data_fcn/train/9.jpg') # numpy.ndarray [70, 116]\n",
    "mask = io.imread('data_fcn/train/9_mask.jpg')\n",
    "img = np.expand_dims(img, 0)\n",
    "img = np.expand_dims(img, -1)\n",
    "pred = fcn.predict(img) # numpy.ndarray [1, 70, 116, 2]\n",
    "img = np.squeeze(img)\n",
    "pred = np.argmax(pred, 3) # [1, 70, 116]\n",
    "pred = np.squeeze(pred, 0)\n",
    "\n",
    "plt.figure(figsize = (15, 7))\n",
    "plt.subplot(1,3,1)\n",
    "plt.imshow(img)\n",
    "plt.subplot(1,3,2)\n",
    "plt.imshow(pred)\n",
    "plt.subplot(1,3,3)\n",
    "plt.imshow(mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Mask detection in original images"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

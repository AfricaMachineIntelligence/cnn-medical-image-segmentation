{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# run on gpu0\n",
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple CNN model architecture for classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.layers import Activation, Dropout, Flatten, Dense\n",
    "\n",
    "simple_cnn = Sequential()\n",
    "simple_cnn.add(Conv2D(16, (3, 3), input_shape=(70, 116, 1), padding='same', activation='relu', name='conv1_1'))\n",
    "simple_cnn.add(Conv2D(16, (3, 3), input_shape=(70, 116, 1), padding='same', activation='relu', name='conv1_2'))\n",
    "simple_cnn.add(MaxPooling2D(pool_size=(2, 2), name='pool1'))\n",
    "\n",
    "simple_cnn.add(Conv2D(32, (3, 3), padding='same', activation='relu', name='conv2_1'))\n",
    "simple_cnn.add(Conv2D(32, (3, 3), padding='same', activation='relu', name='conv2_2'))\n",
    "simple_cnn.add(MaxPooling2D(pool_size=(5, 2), name='pool2'))\n",
    "\n",
    "simple_cnn.add(Flatten(name='flatten'))\n",
    "simple_cnn.add(Dense(32, activation='relu', name='fc1'))\n",
    "simple_cnn.add(Dropout(0.5))\n",
    "simple_cnn.add(Dense(1, activation='sigmoid', name='prediction'))\n",
    "\n",
    "from keras import optimizers\n",
    "adam = optimizers.Adam(lr=1e-7, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
    "\n",
    "simple_cnn.compile(loss='binary_crossentropy',\n",
    "              optimizer=adam,\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "simple_cnn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.utils import plot_model\n",
    "plot_model(simple_cnn, to_file='simple_cnn.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read small image files for classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1393544 images belonging to 2 classes.\n",
      "Found 235343 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "batch_size = 256\n",
    "\n",
    "train_datagen = ImageDataGenerator(\n",
    "        rotation_range=180,\n",
    "        horizontal_flip=True,\n",
    "        fill_mode='nearest')\n",
    "\n",
    "test_datagen = ImageDataGenerator(\n",
    "        rotation_range=180, \n",
    "        horizontal_flip=True,\n",
    "        fill_mode='nearest')\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "        'data_simple_cnn_full/train',  \n",
    "        target_size=(70, 116), \n",
    "        batch_size=batch_size,\n",
    "        class_mode='binary',\n",
    "        color_mode='grayscale') \n",
    "\n",
    "validation_generator = test_datagen.flow_from_directory(\n",
    "        'data_simple_cnn_full/validation',\n",
    "        target_size=(70, 116),\n",
    "        batch_size=batch_size,\n",
    "        class_mode='binary',\n",
    "        color_mode='grayscale')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification training for small images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "simple_cnn.load_weights('simple_cnn_weights/simple_cnn_0_7903.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# time-consuming training process, expect to run for a long time\n",
    "simple_cnn.fit_generator(\n",
    "        train_generator,\n",
    "        steps_per_epoch= 1393544//256,\n",
    "        epochs=5,\n",
    "        validation_data=validation_generator,\n",
    "        validation_steps=235343//256)\n",
    "simple_cnn.save_weights('simple_cnn.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image segmention loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import keras.backend as K\n",
    "\n",
    "def binary_crossentropy_with_logits(ground_truth, predictions):\n",
    "    return K.mean(K.binary_crossentropy(ground_truth,\n",
    "                                        predictions,\n",
    "                                        from_logits=True),\n",
    "                  axis=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FCN model architecture for small image segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Conv2D, MaxPooling2D, Conv2DTranspose, Add, Dropout\n",
    "\n",
    "'''\n",
    "simple_cnn = Sequential()\n",
    "simple_cnn.add(Conv2D(16, (3, 3), input_shape=(70, 116, 1), padding='same', activation='relu', name='conv1_1'))\n",
    "simple_cnn.add(Conv2D(16, (3, 3), input_shape=(70, 116, 1), padding='same', activation='relu', name='conv1_2'))\n",
    "simple_cnn.add(MaxPooling2D(pool_size=(2, 2), name='pool1'))\n",
    "\n",
    "simple_cnn.add(Conv2D(32, (3, 3), padding='same', activation='relu', name='conv2_1'))\n",
    "simple_cnn.add(Conv2D(32, (3, 3), padding='same', activation='relu', name='conv2_2'))\n",
    "simple_cnn.add(MaxPooling2D(pool_size=(5, 2), name='pool2'))\n",
    "\n",
    "simple_cnn.add(Flatten(name='flatten'))\n",
    "simple_cnn.add(Dense(32, activation='relu', name='fc1'))\n",
    "simple_cnn.add(Dropout(0.5))\n",
    "simple_cnn.add(Dense(1, activation='sigmoid', name='prediction'))\n",
    "'''\n",
    "def fcn_cnn():\n",
    "    fc_cnn = Sequential() \n",
    "    fc_cnn.add(Conv2D(16, (3, 3), input_shape=(70, 116, 1), padding='same', activation='relu', name='conv1_1'))\n",
    "    fc_cnn.add(Conv2D(16, (3, 3), input_shape=(70, 116, 1), padding='same', activation='relu', name='conv1_2'))\n",
    "    fc_cnn.add(MaxPooling2D(pool_size=(2, 2), name='pool1')) # (35, 58, 1)\n",
    "\n",
    "    fc_cnn.add(Conv2D(32, (3, 3), padding='same', activation='relu', name='conv2_1'))\n",
    "    fc_cnn.add(Conv2D(32, (3, 3), padding='same', activation='relu', name='conv2_2'))\n",
    "    fc_cnn.add(MaxPooling2D(pool_size=(5, 2), name='pool2')) # (7, 29, 1)\n",
    "\n",
    "    # continue to use convoluational layers instead of fully connected layers\n",
    "    fc_cnn.add(Conv2D(128, (7, 29), padding='same', activation='relu', name='fc3'))\n",
    "    fc_cnn.add(Dropout(0.5))\n",
    "    fc_cnn.add(Conv2D(128, (1, 1), padding='same', activation='relu', name='fc4'))\n",
    "    fc_cnn.add(Dropout(0.5))\n",
    "\n",
    "    fc_cnn.add(Conv2D(2, (1, 1), padding='same', name='logit_fc4')) # [7, 29, 2]\n",
    "    # deconv_logit_fc4 by factor [5, 2] to [35, 58, 2]\n",
    "    fc_cnn.add(Conv2DTranspose(2, kernel_size=(2*5-5%2, 2*2-2%2), strides=(5, 2), padding='same', name='deconv_logit_fc4'))\n",
    "\n",
    "    # conv logit from pool1 to [35, 58, 2]\n",
    "    logit_pool1 = Conv2D(2, (1, 1), padding='same', name='logit_pool1')(fc_cnn.layers[2].output)\n",
    "    # add deconv_logits_fc4 and logit_pool1\n",
    "    logit_pool1_deconv_logit_fc4 = Add()([logit_pool1, fc_cnn.layers[-1].output])\n",
    "    # deconv above sum by [2, 2] to [70, 116, 2] --> pixel-wise classification: segmentation logit\n",
    "    final_deconv = Conv2DTranspose(2, kernel_size=(2*2-2%2, 2*2-2%2), strides=(2, 2), \n",
    "                                   padding='same', name='final_deconv')(logit_pool1_deconv_logit_fc4)\n",
    "    \n",
    "    return Model(fc_cnn.input, final_deconv)\n",
    "\n",
    "fcn = fcn_cnn()\n",
    "\n",
    "adam = optimizers.Adam(lr=1e-5, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
    "fcn.compile(loss=binary_crossentropy_with_logits,\n",
    "              optimizer=adam,\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_cnn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# look up simple_cnn weights, do not need to run\n",
    "\n",
    "layers = simple_cnn.layers\n",
    "for i in range(len(layers)):\n",
    "    n = len(layers[i].get_weights())\n",
    "    print(str(i) + \"-layer weight len: \", n)\n",
    "    if n == 2:\n",
    "        print(\"  weight matrix size: \", layers[i].get_weights()[0].shape)\n",
    "        print(\"  bias vector size: \", layers[i].get_weights()[1].shape)\n",
    "        print(layers[i].get_weights()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fcn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.utils import plot_model\n",
    "plot_model(fcn, to_file='fcn.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# look up fcn weights, do not need to run\n",
    "\n",
    "layers = fcn.layers\n",
    "for i in range(len(layers)):\n",
    "    n = len(layers[i].get_weights())\n",
    "    print(str(i) + \"-layer weight len: \", n)\n",
    "    if n == 2:\n",
    "        print(\"  weight matrix size: \", layers[i].get_weights()[0].shape)\n",
    "        print(\"  bias vector size: \", layers[i].get_weights()[1].shape)\n",
    "        print(layers[i].get_weights()[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize weights in fcn from those in simple_cnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_cnn.load_weights('simple_cnn.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def set_weights(fcn, simple_cnn):\n",
    "    for i in range(5):\n",
    "        fcn.layers[i+1].set_weights(simple_cnn.layers[i].get_weights())\n",
    "        \n",
    "set_weights(fcn, simple_cnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# look up simple_cnn initialized weights, do not need to run\n",
    "\n",
    "layers = fcn.layers\n",
    "for i in range(len(layers)):\n",
    "    n = len(layers[i].get_weights())\n",
    "    print(str(i) + \"-layer weight len: \", n)\n",
    "    if n == 2:\n",
    "        print(\"  weight matrix size: \", layers[i].get_weights()[0].shape)\n",
    "        print(\"  bias vector size: \", layers[i].get_weights()[1].shape)\n",
    "        print(layers[i].get_weights()[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read small image and mask files for segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read training data\n",
    "\n",
    "import skimage.io as io\n",
    "import numpy as np\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "batch_size = 256\n",
    "\n",
    "data_gen_args = dict(featurewise_center=True,\n",
    "                     featurewise_std_normalization=True,\n",
    "                     rotation_range=180.,\n",
    "                     horizontal_flip=True,\n",
    "                     fill_mode='nearest')\n",
    "image_datagen = ImageDataGenerator(**data_gen_args)\n",
    "mask_datagen = ImageDataGenerator(**data_gen_args)\n",
    "\n",
    "seed = 1\n",
    "imgs_filename = [\"data_fcn_full/train/images/images/\"+str(i)+\".jpg\" for i in range(1, 14401)]\n",
    "masks_filename = [\"data_fcn_full/train/masks/masks/\"+str(i)+\"_mask.jpg\" for i in range(1, 14401)]\n",
    "sample_imgs = [np.expand_dims(io.imread(img_name), -1) for img_name in imgs_filename]\n",
    "sample_masks = [np.expand_dims(io.imread(mask_name), -1) for mask_name in masks_filename]\n",
    "image_datagen.fit(sample_imgs, augment=True, seed=seed)\n",
    "mask_datagen.fit(sample_masks, augment=True, seed=seed)\n",
    "\n",
    "image_generator = image_datagen.flow_from_directory(\n",
    "    'data_fcn_full/train/images',\n",
    "    target_size=(70, 116),\n",
    "    batch_size=batch_size,\n",
    "    class_mode=None,\n",
    "    seed=seed,\n",
    "    color_mode='grayscale')\n",
    "\n",
    "mask_generator = mask_datagen.flow_from_directory(\n",
    "    'data_fcn_full/train/masks',\n",
    "    target_size=(70, 116),\n",
    "    batch_size=batch_size,\n",
    "    class_mode=None,\n",
    "    seed=seed,\n",
    "    color_mode='grayscale')\n",
    "\n",
    "train_generator = zip(image_generator, mask_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read validation data\n",
    "\n",
    "data_gen_args = dict(featurewise_center=True,\n",
    "                     featurewise_std_normalization=True,\n",
    "                     rotation_range=180.,\n",
    "                     horizontal_flip=True,\n",
    "                     fill_mode='nearest')\n",
    "image_datagen = ImageDataGenerator(**data_gen_args)\n",
    "mask_datagen = ImageDataGenerator(**data_gen_args)\n",
    "\n",
    "seed = 1\n",
    "imgs_filename = [\"data_fcn_full/validation/images/images/\"+str(i)+\".jpg\" for i in range(1, 3571)]\n",
    "masks_filename = [\"data_fcn_full/validation/masks/masks/\"+str(i)+\"_mask.jpg\" for i in range(1, 3571)]\n",
    "sample_imgs = [np.expand_dims(io.imread(img_name), -1) for img_name in imgs_filename]\n",
    "sample_masks = [np.expand_dims(io.imread(mask_name), -1) for mask_name in masks_filename]\n",
    "image_datagen.fit(sample_imgs, augment=True, seed=seed)\n",
    "mask_datagen.fit(sample_masks, augment=True, seed=seed)\n",
    "\n",
    "image_generator = image_datagen.flow_from_directory(\n",
    "    'data_fcn_full/validation/images',\n",
    "    target_size=(70, 116),\n",
    "    batch_size=batch_size,\n",
    "    class_mode=None,\n",
    "    seed=seed,\n",
    "    color_mode='grayscale')\n",
    "\n",
    "mask_generator = mask_datagen.flow_from_directory(\n",
    "    'data_fcn_full/validation/masks',\n",
    "    target_size=(70, 116),\n",
    "    batch_size=batch_size,\n",
    "    class_mode=None,\n",
    "    seed=seed,\n",
    "    color_mode='grayscale')\n",
    "\n",
    "validation_generator = zip(image_generator, mask_generator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FCN model training on small images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# time-consuming training process of fcn, expect to run for a long time\n",
    "\n",
    "fcn.fit_generator(\n",
    "    train_generator,\n",
    "    steps_per_epoch=160000//256,\n",
    "    epochs=10,\n",
    "    validation_data=validation_generator,\n",
    "    validation_steps=19000//256)\n",
    "fcn.save_weights('fcn.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FCN model prediction on small images demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import skimage.io as io\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "img = io.imread('data_fcn/validation/images/images/74.jpg') # numpy.ndarray [70, 116]\n",
    "mask = io.imread('data_fcn/validation/masks/masks/74_mask.jpg')\n",
    "img = np.expand_dims(img, 0)\n",
    "img = np.expand_dims(img, -1)\n",
    "pred = fcn.predict(img) # numpy.ndarray [1, 70, 116, 2]\n",
    "\n",
    "# print(pred[0, :10, :10, :])\n",
    "\n",
    "img = np.squeeze(img)\n",
    "pred = np.argmax(pred, 3) # numpy.ndarray [1, 70, 116]\n",
    "pred = np.squeeze(pred, 0) # numpy.ndaaray [70, 116]\n",
    "\n",
    "# print(pred[:10, :10])\n",
    "\n",
    "plt.figure(figsize = (15, 7))\n",
    "plt.subplot(1,3,1)\n",
    "plt.imshow(img)\n",
    "plt.subplot(1,3,2)\n",
    "plt.imshow(pred)\n",
    "plt.subplot(1,3,3)\n",
    "plt.imshow(mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mask detection in original images"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  },
  "widgets": {
   "state": {},
   "version": "1.1.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

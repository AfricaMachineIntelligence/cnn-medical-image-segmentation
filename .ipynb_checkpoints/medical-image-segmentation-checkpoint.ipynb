{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import glob\n",
    "import skimage.io as io\n",
    "from PIL import Image\n",
    "\n",
    "import os.path\n",
    "import sys\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib import slim\n",
    "\n",
    "\n",
    "sys.path.append(\"models/research/slim/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get image and mask pair file names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "origin_images = [img for img in glob.glob(\"train_subset/*.tif\") if 'mask' not in img]\n",
    "\n",
    "def fimg_to_fmask(img_path):\n",
    "    # convert an image file path into a corresponding mask file path \n",
    "    dirname, basename = os.path.split(img_path)\n",
    "    maskname = basename.replace(\".tif\", \"_mask.tif\")\n",
    "    return os.path.join(dirname, maskname)\n",
    "\n",
    "paired_images = [(img, fimg_to_fmask(img)) for img in origin_images]\n",
    "\n",
    "# check an image instance\n",
    "img = io.imread('train_subset/1_1.tif')\n",
    "print(type(img))\n",
    "print(img.shape)\n",
    "io.imshow(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construct Tfrecords binary data file\n",
    "\n",
    "Adapt code from this blog http://warmspringwinds.github.io/tensorflow/tf-slim/2016/12/21/tfrecords-guide/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _bytes_feature(value):\n",
    "    return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))\n",
    "\n",
    "def _int64_feature(value):\n",
    "    return tf.train.Feature(int64_list=tf.train.Int64List(value=[value]))\n",
    "\n",
    "tfrecords_filename = 'medical_image_segmentation.tfrecords'\n",
    "\n",
    "writer = tf.python_io.TFRecordWriter(tfrecords_filename)\n",
    "\n",
    "original_images = []\n",
    "\n",
    "for img_path, segmentation_path in paired_images:\n",
    "    \n",
    "    img = np.array(Image.open(img_path))\n",
    "    seg = np.array(Image.open(segmentation_path))\n",
    "        \n",
    "    height = img.shape[0]\n",
    "    width = img.shape[1]\n",
    "    \n",
    "    original_images.append((img, seg))\n",
    "    \n",
    "    img_raw = img.tostring()\n",
    "    seg_raw = seg.tostring()\n",
    "    \n",
    "    example = tf.train.Example(features=tf.train.Features(feature={\n",
    "        'height': _int64_feature(height),\n",
    "        'width': _int64_feature(width),\n",
    "        'image_raw': _bytes_feature(img_raw),\n",
    "        'mask_raw': _bytes_feature(seg_raw)}))\n",
    "    \n",
    "    writer.write(example.SerializeToString())\n",
    "\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check images\n",
    "check reconstructed images from tfrecords file match the original images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reconstructed_images = []\n",
    "\n",
    "record_iterator = tf.python_io.tf_record_iterator(path=tfrecords_filename)\n",
    "\n",
    "for string_record in record_iterator:\n",
    "    \n",
    "    example = tf.train.Example()\n",
    "    example.ParseFromString(string_record)\n",
    "    \n",
    "    height = int(example.features.feature['height']\n",
    "                                 .int64_list\n",
    "                                 .value[0])\n",
    "    \n",
    "    width = int(example.features.feature['width']\n",
    "                                .int64_list\n",
    "                                .value[0])\n",
    "    \n",
    "    img_string = (example.features.feature['image_raw']\n",
    "                                  .bytes_list\n",
    "                                  .value[0])\n",
    "    \n",
    "    seg_string = (example.features.feature['mask_raw']\n",
    "                                .bytes_list\n",
    "                                .value[0])\n",
    "    \n",
    "    img_1d = np.fromstring(img_string, dtype=np.uint8)\n",
    "    reconstructed_img = img_1d.reshape((height, width))\n",
    "    \n",
    "    seg_1d = np.fromstring(seg_string, dtype=np.uint8)\n",
    "    \n",
    "    reconstructed_seg = seg_1d.reshape((height, width))\n",
    "    \n",
    "    reconstructed_images.append((reconstructed_img, reconstructed_seg))\n",
    "    \n",
    "# check if the reconstructed images match the original images\n",
    "\n",
    "for original_pair, reconstructed_pair in zip(original_images, reconstructed_images):\n",
    "    \n",
    "    img_pair_to_compare, seg_pair_to_compare = zip(original_pair,\n",
    "                                                          reconstructed_pair)\n",
    "    print(np.allclose(*img_pair_to_compare))\n",
    "    print(np.allclose(*seg_pair_to_compare))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load data from tfrecords file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def load_batch(tfrecords_filename, capacity, img_height, img_width, batch_size=32, num_epochs=10, is_training=False):\n",
    "    \n",
    "    filename_queue = tf.train.string_input_producer([tfrecords_filename], num_epochs=num_epochs)\n",
    "\n",
    "    \n",
    "    reader = tf.TFRecordReader()\n",
    "\n",
    "    _, serialized_example = reader.read(filename_queue)\n",
    "\n",
    "    features = tf.parse_single_example(\n",
    "      serialized_example,\n",
    "      features={\n",
    "        'height': tf.FixedLenFeature([], tf.int64),\n",
    "        'width': tf.FixedLenFeature([], tf.int64),\n",
    "        'image_raw': tf.FixedLenFeature([], tf.string),\n",
    "        'mask_raw': tf.FixedLenFeature([], tf.string)\n",
    "        })\n",
    "\n",
    "    image = tf.decode_raw(features['image_raw'], tf.uint8)\n",
    "    segmentation = tf.decode_raw(features['mask_raw'], tf.uint8)\n",
    "    \n",
    "    height = tf.cast(features['height'], tf.int32)\n",
    "    width = tf.cast(features['width'], tf.int32)\n",
    "    \n",
    "    image_shape = tf.stack([height, width])\n",
    "    segmentation_shape = tf.stack([height, width])\n",
    "    \n",
    "    image = tf.reshape(image, image_shape)\n",
    "    image = tf.expand_dims(image, -1)\n",
    "    segmentation = tf.reshape(segmentation, segmentation_shape)\n",
    "    segmentation = tf.expand_dims(segmentation, -1)\n",
    "        \n",
    "    image_size_const = tf.constant((img_height, img_width, 1), dtype=tf.int32)\n",
    "    segmentation_size_const = tf.constant((img_height, img_width, 1), dtype=tf.int32)\n",
    "\n",
    "    images, segmentations = tf.train.shuffle_batch([image, segmentation],\n",
    "                                                 batch_size=batch_size,\n",
    "                                                 capacity=capacity,\n",
    "                                                 num_threads=2,\n",
    "                                                 min_after_dequeue=10,\n",
    "                                                shapes=[[img_height, img_width, 1], [img_height, img_width, 1]])\n",
    "    \n",
    "    return images, segmentations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### loading data from tfrecords file demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "TFRECORDS_FILENAME = 'medical_image_segmentation.tfrecords'\n",
    "IMAGE_HEIGHT = 420\n",
    "IMAGE_WIDTH = 580\n",
    "CAPACITY = 599\n",
    "\n",
    "images, segmentations = load_batch(TFRECORDS_FILENAME, capacity=CAPACITY, img_height=IMAGE_HEIGHT, img_width=IMAGE_WIDTH,\n",
    "                                   batch_size=32, num_epochs=10, is_training=True)\n",
    "\n",
    "\n",
    "init_op = tf.group(tf.global_variables_initializer(),\n",
    "                   tf.local_variables_initializer())\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    \n",
    "    sess.run(init_op)\n",
    "    \n",
    "    coord = tf.train.Coordinator()\n",
    "    threads = tf.train.start_queue_runners(coord=coord)\n",
    "    \n",
    "    for i in range(3):\n",
    "    \n",
    "        imgs, segs = sess.run([images, segmentations])\n",
    "        \n",
    "        imgs = np.concatenate([imgs, imgs, imgs], axis=3)\n",
    "        segs = np.concatenate([segs, segs, segs], axis=3)\n",
    "\n",
    "        \n",
    "        print(imgs[0, :, :, :].shape)\n",
    "        \n",
    "        print('current batch')\n",
    "        \n",
    "        for j in range(3):\n",
    "        \n",
    "            io.imshow(imgs[j, :, :, :])\n",
    "            io.show()\n",
    "\n",
    "            io.imshow(segs[j, :, :, :])\n",
    "            io.show()\n",
    "        \n",
    "    coord.request_stop()\n",
    "    coord.join(threads)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### downsampling and upsampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_kernel_size(factors):\n",
    "    \"\"\"\n",
    "    Find the kernel size given the desired factor of upsampling.\n",
    "    \"\"\"\n",
    "    return [2 * factor - factor % 2 for factor in factors]\n",
    "\n",
    "\n",
    "def upsample_filt(sizes):\n",
    "    \"\"\"\n",
    "    Make a 2D bilinear kernel suitable for upsampling of the given (h, w) size.\n",
    "    \"\"\"\n",
    "    factors = [(size + 1) // 2 for size in sizes]\n",
    "    center = [0]*len(sizes)\n",
    "    for i in range(len(sizes)):\n",
    "        if sizes[i] % 2 == 1:\n",
    "            center[i] = factors[i] - 1\n",
    "        else:\n",
    "            center[i] = factors[i] - 0.5\n",
    "        og = np.ogrid[:sizes[0], :sizes[1]]\n",
    "    return (1 - abs(og[0] - center[0]) / factors[0]) * (1 - abs(og[1] - center[1]) / factors[1])\n",
    "\n",
    "\n",
    "def bilinear_upsample_weights(factors, number_of_classes):\n",
    "    \"\"\"\n",
    "    Create weights matrix for transposed convolution with bilinear filter\n",
    "    initialization.\n",
    "    \"\"\"\n",
    "    \n",
    "    filter_sizes = get_kernel_size(factors)\n",
    "    \n",
    "    weights = np.zeros((filter_sizes[0],\n",
    "                        filter_sizes[1],\n",
    "                        number_of_classes,\n",
    "                        number_of_classes), dtype=np.float32)\n",
    "    \n",
    "    upsample_kernel = upsample_filt(filter_sizes)\n",
    "    \n",
    "    for i in range(number_of_classes):\n",
    "        \n",
    "        weights[:, :, i, i] = upsample_kernel\n",
    "    \n",
    "    return weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN model architecture\n",
    "\n",
    "- Fully Convolutional Network: https://www.cv-foundation.org/openaccess/content_cvpr_2015/html/Long_Fully_Convolutional_Networks_2015_CVPR_paper.html\n",
    "\n",
    "- One FCN-8 model implementation: https://github.com/warmspringwinds/tf-image-segmentation\n",
    "\n",
    "- Tensorflow slim VGG16 model with FCN feature: https://github.com/tensorflow/models/blob/master/research/slim/nets/vgg.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def vgg_arg_scope(weight_decay=0.0005):\n",
    "  with slim.arg_scope([slim.conv2d],\n",
    "                      activation_fn=tf.nn.relu,\n",
    "                      weights_regularizer=slim.l2_regularizer(weight_decay),\n",
    "                      biases_initializer=tf.zeros_initializer(), padding='SAME') as arg_sc:\n",
    "      return arg_sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def vgg(inputs, num_classes=2, is_training=True, dropout_keep_prob=0.5, scope='vgg', fc_conv_padding='SAME'):\n",
    "  with tf.variable_scope(scope, 'vgg', [inputs]) as sc:\n",
    "    end_points_collection = sc.name + '_end_points'\n",
    "    with slim.arg_scope([slim.conv2d, slim.max_pool2d], outputs_collections=end_points_collection):\n",
    "      # [420, 580, 1]\n",
    "      net = slim.repeat(inputs, 2, slim.conv2d, 64, [3, 3], scope='conv1') # [420, 580, 64]\n",
    "      net = slim.max_pool2d(net, [2, 2], scope='pool1') # [210, 290, 64]\n",
    "      net = slim.repeat(net, 2, slim.conv2d, 128, [3, 3], scope='conv2') # [210, 290, 128]\n",
    "      net = slim.max_pool2d(net, [2, 2], scope='pool2') # [105, 145, 128]\n",
    "      net = slim.repeat(net, 3, slim.conv2d, 256, [3, 3], scope='conv3') # [105, 145, 256]\n",
    "      net = slim.max_pool2d(net, [3, 5], scope='pool3') # [35, 29, 256]\n",
    "      net = slim.repeat(net, 3, slim.conv2d, 512, [3, 3], scope='conv4') # [35, 29, 512]\n",
    "      net = slim.max_pool2d(net, [5, 1], scope='pool4') # [7, 29, 512] \n",
    "      # convolution effect: 60*20  \n",
    "      # Use conv2d instead of fully_connected layers.\n",
    "      net = slim.conv2d(net, 1024, [7, 29], padding=fc_conv_padding, scope='fc5') # [7,29, 1024]\n",
    "      net = slim.dropout(net, dropout_keep_prob, is_training=is_training,\n",
    "                         scope='dropout5')\n",
    "      net = slim.conv2d(net, 1024, [1, 1], scope='fc6') # [7, 29, 1024]\n",
    "      net = slim.dropout(net, dropout_keep_prob, is_training=is_training,\n",
    "                         scope='dropout6')\n",
    "      net = slim.conv2d(net, num_classes, [1, 1],\n",
    "                        activation_fn=None,\n",
    "                        normalizer_fn=None,\n",
    "                        scope='fc7') # [7, 29, 2]\n",
    "      # Convert end_points_collection into an end_point dict.\n",
    "      end_points = slim.utils.convert_collection_to_dict(end_points_collection)\n",
    "      return net, end_points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fcn(image_batch_tensor, number_of_classes=2, is_training=False):\n",
    "\n",
    "    image_batch_float = tf.to_float(image_batch_tensor)\n",
    "\n",
    "    upsample_filter_factor_5_1_np = bilinear_upsample_weights(factors=[5, 1], number_of_classes=number_of_classes)\n",
    "    upsample_filter_factor_3_5_np = bilinear_upsample_weights(factors=[3, 5], number_of_classes=number_of_classes)    \n",
    "    upsample_filter_factor_4_4_np = bilinear_upsample_weights(factors=[4, 4], number_of_classes=number_of_classes)\n",
    "\n",
    "    upsample_filter_factor_5_1_tensor = tf.constant(upsample_filter_factor_5_1_np)\n",
    "    upsample_filter_factor_3_5_tensor = tf.constant(upsample_filter_factor_3_5_np)\n",
    "    upsample_filter_factor_4_4_tensor = tf.constant(upsample_filter_factor_4_4_np)\n",
    "\n",
    "    with tf.variable_scope(\"fcn\")  as fcn_scope:\n",
    "        with slim.arg_scope(vgg_arg_scope()):\n",
    "            last_layer_logits, end_points = vgg(image_batch_float,\n",
    "                                                       num_classes=number_of_classes,\n",
    "                                                       is_training=is_training,\n",
    "                                                       fc_conv_padding='SAME')\n",
    "            last_layer_logits_shape = tf.shape(last_layer_logits)\n",
    "            last_layer_upsampled_by_factor_5_1_logits_shape = tf.stack([\n",
    "                                                                  last_layer_logits_shape[0],\n",
    "                                                                  last_layer_logits_shape[1] * 5,\n",
    "                                                                  last_layer_logits_shape[2] * 1,\n",
    "                                                                  last_layer_logits_shape[3]\n",
    "                                                                 ])\n",
    "            last_layer_upsampled_by_factor_5_1_logits = tf.nn.conv2d_transpose(last_layer_logits,\n",
    "                                                                             upsample_filter_factor_5_1_tensor,\n",
    "                                                                             output_shape=last_layer_upsampled_by_factor_5_1_logits_shape,\n",
    "                                                                             strides=[1, 5, 1, 1])\n",
    "\n",
    "            \n",
    "            pool3_features = end_points['fcn/vgg/pool3']\n",
    "            pool3_logits = slim.conv2d(pool3_features,\n",
    "                                       number_of_classes,\n",
    "                                       [1, 1],\n",
    "                                       activation_fn=None,\n",
    "                                       normalizer_fn=None,\n",
    "                                       weights_initializer=tf.zeros_initializer,\n",
    "                                       scope='pool3_fc')\n",
    "            fused_last_layer_and_pool3_logits = pool3_logits + last_layer_upsampled_by_factor_5_1_logits\n",
    "            fused_last_layer_and_pool3_logits_shape = tf.shape(fused_last_layer_and_pool3_logits)\n",
    "            fused_last_layer_and_pool3_upsampled_by_factor_3_5_logits_shape = tf.stack([\n",
    "                                                                          fused_last_layer_and_pool3_logits_shape[0],\n",
    "                                                                          fused_last_layer_and_pool3_logits_shape[1] * 3,\n",
    "                                                                          fused_last_layer_and_pool3_logits_shape[2] * 5,\n",
    "                                                                          fused_last_layer_and_pool3_logits_shape[3]\n",
    "                                                                         ])\n",
    "            fused_last_layer_and_pool3_upsampled_by_factor_3_5_logits = tf.nn.conv2d_transpose(fused_last_layer_and_pool3_logits,\n",
    "                                                                        upsample_filter_factor_3_5_tensor,\n",
    "                                                                        output_shape=fused_last_layer_and_pool3_upsampled_by_factor_3_5_logits_shape,\n",
    "                                                                        strides=[1, 3, 5, 1])\n",
    "            \n",
    "            \n",
    "            pool2_features = end_points['fcn/vgg/pool2']\n",
    "            pool2_logits = slim.conv2d(pool2_features,\n",
    "                                       number_of_classes,\n",
    "                                       [1, 1],\n",
    "                                       activation_fn=None,\n",
    "                                       normalizer_fn=None,\n",
    "                                       weights_initializer=tf.zeros_initializer,\n",
    "                                       scope='pool2_fc')\n",
    "            fused_last_layer_and_pool3_logits_and_pool_2_logits = pool2_logits + \\\n",
    "                                            fused_last_layer_and_pool3_upsampled_by_factor_3_5_logits\n",
    "            fused_last_layer_and_pool3_logits_and_pool_2_logits_shape = tf.shape(fused_last_layer_and_pool3_logits_and_pool_2_logits)\n",
    "            fused_last_layer_and_pool3_logits_and_pool_2_upsampled_by_factor_4_4_logits_shape = tf.stack([\n",
    "                                                                          fused_last_layer_and_pool3_logits_and_pool_2_logits_shape[0],\n",
    "                                                                          fused_last_layer_and_pool3_logits_and_pool_2_logits_shape[1] * 4,\n",
    "                                                                          fused_last_layer_and_pool3_logits_and_pool_2_logits_shape[2] * 4,\n",
    "                                                                          fused_last_layer_and_pool3_logits_and_pool_2_logits_shape[3]\n",
    "                                                                         ])\n",
    "            fused_last_layer_and_pool3_logits_and_pool_2_upsampled_by_factor_4_4_logits = tf.nn.conv2d_transpose(fused_last_layer_and_pool3_logits_and_pool_2_logits,\n",
    "                                                                        upsample_filter_factor_4_4_tensor,\n",
    "                                                                        output_shape=fused_last_layer_and_pool3_logits_and_pool_2_upsampled_by_factor_4_4_logits_shape,\n",
    "                                                                        strides=[1, 4, 4, 1])\n",
    "            \n",
    "            \n",
    "            fcn_5_1_variables_mapping = {}\n",
    "            fcn_variables = slim.get_variables(fcn_scope)\n",
    "            for variable in fcn_variables:\n",
    "                if 'pool2_fc' in variable.name:\n",
    "                    continue\n",
    "                original_fcn_5_1_checkpoint_string = 'fcn/' +  variable.name[len(fcn_scope.original_name_scope):-2]\n",
    "                fcn_5_1_variables_mapping[original_fcn_5_1_checkpoint_string] = variable\n",
    "\n",
    "                \n",
    "    return fused_last_layer_and_pool3_logits_and_pool_2_upsampled_by_factor_4_4_logits, fcn_5_1_variables_mapping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TFRECORDS_FILENAME = 'medical_image_segmentation.tfrecords'\n",
    "IMAGE_HEIGHT = 420\n",
    "IMAGE_WIDTH = 580\n",
    "CAPACITY = 21\n",
    "BATCH_SIZE= 2\n",
    "NUM_EPOCHS = CAPACITY / BATCH_SIZE + 1\n",
    "NUMBER_OF_CLASSES = 2\n",
    "LEARNING_RATE = 0.1\n",
    "\n",
    "train_log_dir = 'fcn_model_checkpoints/'\n",
    "if not tf.gfile.Exists(train_log_dir):\n",
    "  tf.gfile.MakeDirs(train_log_dir)\n",
    "print('Will save model to %s' % train_log_dir)\n",
    "\n",
    "with tf.Graph().as_default():\n",
    "    #tf.logging.set_verbosity(tf.logging.INFO)\n",
    "\n",
    "    images, segmentations = load_batch(TFRECORDS_FILENAME, capacity=CAPACITY, \n",
    "                                       img_height=IMAGE_HEIGHT, img_width=IMAGE_WIDTH,\n",
    "                                       batch_size=BATCH_SIZE, num_epochs=NUM_EPOCHS, is_training=True)\n",
    "            \n",
    "    # print('images shape: ', images.shape)\n",
    "\n",
    "    logits, _ = fcn(image_batch_tensor=images,\n",
    "                    number_of_classes=NUMBER_OF_CLASSES,\n",
    "                    is_training=True)\n",
    "    \n",
    "    flat_logits = tf.reshape(tensor=logits, shape=(-1, NUMBER_OF_CLASSES))\n",
    "    flat_segs = tf.reshape(tensor=segmentations, shape=(-1, NUMBER_OF_CLASSES))\n",
    "    # print(\"logits shape: \", flat_logits.shape)\n",
    "    # print(\"labels shape: \", flat_segs)\n",
    "    cross_entropies = tf.nn.softmax_cross_entropy_with_logits(logits=flat_logits, labels=flat_segs)\n",
    "    cross_entropy_sum = tf.reduce_sum(cross_entropies)\n",
    "\n",
    "    tf.summary.scalar('losses/Total_Loss', cross_entropy_sum)\n",
    "  \n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=LEARNING_RATE)\n",
    "    train_op = slim.learning.create_train_op(cross_entropy_sum, optimizer)\n",
    "\n",
    "    # Run the training:\n",
    "    final_loss = slim.learning.train(\n",
    "        train_op,\n",
    "        logdir=train_log_dir,\n",
    "        number_of_steps=1,\n",
    "        save_summaries_secs=2,\n",
    "        save_interval_secs=2)\n",
    "  \n",
    "    print('Finished training. Final batch loss %d' % final_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Testing"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
